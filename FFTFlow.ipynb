{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries we need\n",
    "import tensorflow as tf\n",
    "#TFP bijector class and PDFs aff all sorts\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer, Dense, ReLU\n",
    "#Model to save the model otherwise I had problems\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "#Shortnames\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "K = tf.keras\n",
    "#Change datatype here for extra precision\n",
    "DTYPE=tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##NN layer inside the RNVP (scale and shift)\n",
    "class NN(Layer):\n",
    "  \n",
    "  def __init__(self,input_shape,output_shape,n_hidden = [512,512],activation = 'relu', name = 'NN'):\n",
    "    super(NN,self).__init__(name = name)\n",
    "    layers = []\n",
    "    for layer, parameters in enumerate(n_hidden):\n",
    "      layers.append(Dense(parameters, activation = activation, name='dense_1_{}'.format(layer), kernel_initializer='glorot_normal',bias_initializer='zeros'))\n",
    "    self.layer_list = layers\n",
    "    #I played with initializers you might want to change them to something like uniform of normal\n",
    "    self.s_layer = Dense(output_shape, activation ='tanh', name = 's',kernel_initializer='zeros',bias_initializer='zeros')\n",
    "    self.t_layer = Dense(output_shape, activation = 'linear',name='t',kernel_initializer='zeros',bias_initializer='zeros')\n",
    "    \n",
    "  def call(self,x):\n",
    "    input = x\n",
    "    for layer in self.layer_list:\n",
    "      input = layer(input)\n",
    "    s = self.s_layer(input)\n",
    "    t = self.t_layer(input)\n",
    "    return s, t\n",
    "\n",
    "\n",
    "#tfp realisazion of RNVP bijection layer Just put them in tfb Chain \n",
    "class RealNVP(tfb.Bijector):\n",
    "  \n",
    "  def __init__(self, input_shape, split = 2, n_hidden = [1024, 1024], validate_args=False, name = 'RealNVP'):\n",
    "    super(RealNVP, self).__init__(\n",
    "            forward_min_event_ndims=1,\n",
    "            inverse_min_event_ndims=1,\n",
    "            validate_args=validate_args,\n",
    "            name=name\n",
    "    )\n",
    "    NN_input_shape = split[0]\n",
    "    NN_output_shape = split[1]\n",
    "    Network = NN(NN_input_shape,NN_output_shape, n_hidden)\n",
    "    flowing_x = tf.keras.Input(NN_input_shape)\n",
    "    s, t = Network(flowing_x)\n",
    "    self.flowing_st = Model(flowing_x,[s,t],name='s_and_t_parametrized')\n",
    "    self.split = split\n",
    "  def _g_linear(self,x):\n",
    "    s,t = self.flowing_st(x)\n",
    "    return tfb.Chain([tfb.Shift(t),tfb.Scale(log_scale=s)])\n",
    "  \n",
    "  def _forward(self,x):\n",
    "    x_1d, x_dD = tf.split(x,self.split,axis=-1)\n",
    "    y_1d = x_1d\n",
    "    y_dD = self._g_linear(x_1d).forward(x_dD)\n",
    "    return tf.concat([y_1d,y_dD], axis = -1)\n",
    "  \n",
    "  def _inverse(self,y):\n",
    "    y_1d, y_dD = tf.split(y,self.split,axis=-1)\n",
    "    x_1d = y_1d\n",
    "    x_dD = self._g_linear(y_1d).inverse(y_dD)\n",
    "    return tf.concat([x_1d,x_dD], axis = -1)\n",
    "\n",
    "  def _forward_log_det_jacobian(self, x):\n",
    "    x_1d, x_dD = tf.split(x, self.split, axis=-1)\n",
    "    return self._g_linear(x_1d).forward_log_det_jacobian(x_dD, event_ndims=1)\n",
    "\n",
    "  def _inverse_log_det_jacobian(self, y):\n",
    "    y_1d, y_dD = tf.split(y, self.split, axis=-1)\n",
    "    return self._g_linear(y_1d).inverse_log_det_jacobian(y_dD, event_ndims=1)\n",
    "\n",
    "\n",
    "#tfp realisazion of FFT bijection layer Just put them in tfb Chain \n",
    "class RealFFT(tfb.Bijector):\n",
    "  \n",
    "  def __init__(self, input_shape, validate_args=False, name = 'RealFFT',is_constant_jacobian=True):\n",
    "    super(RealFFT, self).__init__(\n",
    "            forward_min_event_ndims=1,\n",
    "            inverse_min_event_ndims=1,\n",
    "            validate_args=validate_args,\n",
    "            name=name\n",
    "    )\n",
    "    self.input_shape = input_shape\n",
    "  \n",
    "  def _forward(self,x):\n",
    "    x1,x1D,xD=tf.split(tf.signal.rfft(x),[1,self.input_shape//2-1,1],axis=-1)\n",
    "    x1=tf.math.real(x1)\n",
    "    x1D_real=tf.math.real(x1D)\n",
    "    x1D_imag=tf.math.imag(x1D)\n",
    "    xD=tf.math.real(xD)\n",
    "    x = tf.concat([x1,x1D_real,x1D_imag,xD],axis=-1)\n",
    "    return x\n",
    "  \n",
    "  def _inverse(self,y):\n",
    "    y1, y1D_real, y1D_imag, yD = tf.split(y,[1,self.input_shape//2-1,self.input_shape//2-1,1],axis=-1)\n",
    "    \n",
    "    y1D_real = tf.concat([y1,y1D_real,yD],axis=-1)\n",
    "\n",
    "    y1D_imag = tf.concat([0*y1,y1D_imag,0*yD],axis=-1)\n",
    "\n",
    "    y = tf.signal.irfft(tf.complex(y1D_real,y1D_imag))\n",
    "\n",
    "    return y\n",
    "\n",
    "  def _inverse_log_det_jacobian(self, y):\n",
    "    return -self._forward_log_det_jacobian(self._inverse(y))\n",
    "\n",
    "  def _forward_log_det_jacobian(self, x):\n",
    "    # The full log jacobian determinant would be tf.zero_like(x).\n",
    "    # However, we circumvent materializing that, since the jacobian\n",
    "    # calculation is input independent, and we specify it for one input.\n",
    "    return tf.constant(42, x.dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Permutation we use\n",
    "#Here its just [1,2,3,4] to [1,3,2,4] this is done because latter we use second half as input for the s and t of the firts\n",
    "input_size=144\n",
    "perm = []\n",
    "for i in np.arange(input_size//2,step=2):\n",
    "  perm+=[i]+[input_size//2+i]\n",
    "\n",
    "for i in np.arange(1,input_size//2,step=2):\n",
    "  perm+=[i]+[input_size//2+i]\n",
    "perm1 = perm\n",
    "perm = []\n",
    "for i in np.arange(input_size//2,input_size,step=1):\n",
    "  perm+=[i]\n",
    "\n",
    "for i in np.arange(input_size//2,step=1):\n",
    "  perm+=[i]\n",
    "perm2=perm\n",
    "perm1[50]\n",
    "\n",
    "mvn2 = tfd.MultivariateNormalDiag(loc=[0.]*input_size)\n",
    "\n",
    "#number of bijection layers\n",
    "num_realnvp = 16\n",
    "bijector_chain = []\n",
    "#sequence of RNVP and permutation layers\n",
    "bijector_chain.append(tfp.bijectors.Invert(RealFFT(input_shape=input_size)))\n",
    "bijector_chain.append(tfp.bijectors.Permute(perm1))\n",
    "for i in range(num_realnvp):\n",
    "    bijector_chain.append(tfp.bijectors.Permute(perm2))\n",
    "    #network parameters\n",
    "    bijector_chain.append(RealNVP(input_shape=input_size, split=[input_size//2,input_size//2],n_hidden=[256,256]))\n",
    "bijector_chain.append(tfp.bijectors.Permute(perm1))\n",
    "bijector_chain.append(RealFFT(input_shape=input_size))\n",
    "\n",
    "flow2 = tfd.TransformedDistribution(\n",
    "    distribution=mvn2,\n",
    "    #we reverse the secuence because its actually applied first bijector in the list first otherwise its in a different direction\n",
    "    bijector=tfb.Chain(list(reversed(bijector_chain)))\n",
    ")\n",
    "print('trainable_variables: ', len(flow2.bijector.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.2)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_12_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.3)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_13_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.4)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_14_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.5)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_15_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.6)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_16_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(2)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "checkpoint_directory=\"C:\\FFT\\\\\"\n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "\n",
    "#status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_30_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
