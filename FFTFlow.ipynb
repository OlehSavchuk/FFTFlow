{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries we need\n",
    "import tensorflow as tf\n",
    "#TFP bijector class and PDFs aff all sorts\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Layer, Dense, ReLU\n",
    "#Model to save the model otherwise I had problems\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "#Shortnames\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "K = tf.keras\n",
    "#Change datatype here for extra precision\n",
    "DTYPE=tf.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##NN layer inside the RNVP (scale and shift)\n",
    "class NN(Layer):\n",
    "  \n",
    "  def __init__(self,input_shape,output_shape,n_hidden = [512,512],activation = 'relu', name = 'NN'):\n",
    "    super(NN,self).__init__(name = name)\n",
    "    layers = []\n",
    "    for layer, parameters in enumerate(n_hidden):\n",
    "      layers.append(Dense(parameters, activation = activation, name='dense_1_{}'.format(layer), kernel_initializer='glorot_normal',bias_initializer='zeros'))\n",
    "    self.layer_list = layers\n",
    "    #I played with initializers you might want to change them to something like uniform of normal\n",
    "    self.s_layer = Dense(output_shape, activation ='tanh', name = 's',kernel_initializer='zeros',bias_initializer='zeros')\n",
    "    self.t_layer = Dense(output_shape, activation = 'linear',name='t',kernel_initializer='zeros',bias_initializer='zeros')\n",
    "    \n",
    "  def call(self,x):\n",
    "    input = x\n",
    "    for layer in self.layer_list:\n",
    "      input = layer(input)\n",
    "    s = self.s_layer(input)\n",
    "    t = self.t_layer(input)\n",
    "    return s, t\n",
    "\n",
    "\n",
    "#tfp realisazion of RNVP bijection layer Just put them in tfb Chain \n",
    "class RealNVP(tfb.Bijector):\n",
    "  \n",
    "  def __init__(self, input_shape, split = 2, n_hidden = [1024, 1024], validate_args=False, name = 'RealNVP'):\n",
    "    super(RealNVP, self).__init__(\n",
    "            forward_min_event_ndims=1,\n",
    "            inverse_min_event_ndims=1,\n",
    "            validate_args=validate_args,\n",
    "            name=name\n",
    "    )\n",
    "    NN_input_shape = split[0]\n",
    "    NN_output_shape = split[1]\n",
    "    Network = NN(NN_input_shape,NN_output_shape, n_hidden)\n",
    "    flowing_x = tf.keras.Input(NN_input_shape)\n",
    "    s, t = Network(flowing_x)\n",
    "    self.flowing_st = Model(flowing_x,[s,t],name='s_and_t_parametrized')\n",
    "    self.split = split\n",
    "  def _g_linear(self,x):\n",
    "    s,t = self.flowing_st(x)\n",
    "    return tfb.Chain([tfb.Shift(t),tfb.Scale(log_scale=s)])\n",
    "  \n",
    "  def _forward(self,x):\n",
    "    x_1d, x_dD = tf.split(x,self.split,axis=-1)\n",
    "    y_1d = x_1d\n",
    "    y_dD = self._g_linear(x_1d).forward(x_dD)\n",
    "    return tf.concat([y_1d,y_dD], axis = -1)\n",
    "  \n",
    "  def _inverse(self,y):\n",
    "    y_1d, y_dD = tf.split(y,self.split,axis=-1)\n",
    "    x_1d = y_1d\n",
    "    x_dD = self._g_linear(y_1d).inverse(y_dD)\n",
    "    return tf.concat([x_1d,x_dD], axis = -1)\n",
    "\n",
    "  def _forward_log_det_jacobian(self, x):\n",
    "    x_1d, x_dD = tf.split(x, self.split, axis=-1)\n",
    "    return self._g_linear(x_1d).forward_log_det_jacobian(x_dD, event_ndims=1)\n",
    "\n",
    "  def _inverse_log_det_jacobian(self, y):\n",
    "    y_1d, y_dD = tf.split(y, self.split, axis=-1)\n",
    "    return self._g_linear(y_1d).inverse_log_det_jacobian(y_dD, event_ndims=1)\n",
    "\n",
    "\n",
    "#tfp realisazion of FFT bijection layer Just put them in tfb Chain \n",
    "class RealFFT(tfb.Bijector):\n",
    "  \n",
    "  def __init__(self, input_shape, validate_args=False, name = 'RealFFT',is_constant_jacobian=True):\n",
    "    super(RealFFT, self).__init__(\n",
    "            forward_min_event_ndims=1,\n",
    "            inverse_min_event_ndims=1,\n",
    "            validate_args=validate_args,\n",
    "            name=name\n",
    "    )\n",
    "    self.input_shape = input_shape\n",
    "  \n",
    "  def _forward(self,x):\n",
    "    x1,x1D,xD=tf.split(tf.signal.rfft(x),[1,self.input_shape//2-1,1],axis=-1)\n",
    "    x1=tf.math.real(x1)\n",
    "    x1D_real=tf.math.real(x1D)\n",
    "    x1D_imag=tf.math.imag(x1D)\n",
    "    xD=tf.math.real(xD)\n",
    "    x = tf.concat([x1,x1D_real,x1D_imag,xD],axis=-1)\n",
    "    return x\n",
    "  \n",
    "  def _inverse(self,y):\n",
    "    y1, y1D_real, y1D_imag, yD = tf.split(y,[1,self.input_shape//2-1,self.input_shape//2-1,1],axis=-1)\n",
    "    \n",
    "    y1D_real = tf.concat([y1,y1D_real,yD],axis=-1)\n",
    "\n",
    "    y1D_imag = tf.concat([0*y1,y1D_imag,0*yD],axis=-1)\n",
    "\n",
    "    y = tf.signal.irfft(tf.complex(y1D_real,y1D_imag))\n",
    "\n",
    "    return y\n",
    "\n",
    "  def _inverse_log_det_jacobian(self, y):\n",
    "    return -self._forward_log_det_jacobian(self._inverse(y))\n",
    "\n",
    "  def _forward_log_det_jacobian(self, x):\n",
    "    # The full log jacobian determinant would be tf.zero_like(x).\n",
    "    # However, we circumvent materializing that, since the jacobian\n",
    "    # calculation is input independent, and we specify it for one input.\n",
    "    return tf.constant(42, x.dtype)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable_variables:  128\n"
     ]
    }
   ],
   "source": [
    "#Permutation we use\n",
    "#Here its just [1,2,3,4] to [1,3,2,4] this is done because latter we use second half as input for the s and t of the firts\n",
    "input_size=144\n",
    "perm = []\n",
    "for i in np.arange(input_size//2,step=2):\n",
    "  perm+=[i]+[input_size//2+i]\n",
    "\n",
    "for i in np.arange(1,input_size//2,step=2):\n",
    "  perm+=[i]+[input_size//2+i]\n",
    "perm1 = perm\n",
    "perm = []\n",
    "for i in np.arange(input_size//2,input_size,step=1):\n",
    "  perm+=[i]\n",
    "\n",
    "for i in np.arange(input_size//2,step=1):\n",
    "  perm+=[i]\n",
    "perm2=perm\n",
    "perm1[50]\n",
    "\n",
    "mvn2 = tfd.MultivariateNormalDiag(loc=[0.]*input_size)\n",
    "\n",
    "#number of bijection layers\n",
    "num_realnvp = 16\n",
    "bijector_chain = []\n",
    "#sequence of RNVP and permutation layers\n",
    "bijector_chain.append(tfp.bijectors.Invert(RealFFT(input_shape=input_size)))\n",
    "bijector_chain.append(tfp.bijectors.Permute(perm1))\n",
    "for i in range(num_realnvp):\n",
    "    bijector_chain.append(tfp.bijectors.Permute(perm2))\n",
    "    #network parameters\n",
    "    bijector_chain.append(RealNVP(input_shape=input_size, split=[input_size//2,input_size//2],n_hidden=[256,256]))\n",
    "bijector_chain.append(tfp.bijectors.Permute(perm1))\n",
    "bijector_chain.append(RealFFT(input_shape=input_size))\n",
    "\n",
    "flow2 = tfd.TransformedDistribution(\n",
    "    distribution=mvn2,\n",
    "    #we reverse the secuence because its actually applied first bijector in the list first otherwise its in a different direction\n",
    "    bijector=tfb.Chain(list(reversed(bijector_chain)))\n",
    ")\n",
    "print('trainable_variables: ', len(flow2.bijector.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.2)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Savch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "1000/1000 [==============================] - 725s 719ms/step - loss: 24.5851\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: 5.0140\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 4.8205\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 4.6799\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 701s 701ms/step - loss: 4.5636\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 4.4670\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 4.3810\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 4.3017\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: 4.2104\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 4.1147\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_12_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "1000/1000 [==============================] - 706s 700ms/step - loss: 5.7944\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: 5.6380\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 703s 703ms/step - loss: 5.4820\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 703s 703ms/step - loss: 5.3335\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: 5.2024\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 701s 701ms/step - loss: 5.0952\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 701s 701ms/step - loss: 5.0107\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 709s 709ms/step - loss: 4.9394\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 4.8844\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: 4.8331\n"
     ]
    }
   ],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.3)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_13_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "1000/1000 [==============================] - 706s 701ms/step - loss: 6.5706\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 706s 706ms/step - loss: 6.5246\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 701s 701ms/step - loss: 6.4820\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 6.4487\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 701s 701ms/step - loss: 6.4183\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 6.3907\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 6.3646\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 6.3425\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 703s 703ms/step - loss: 6.3210\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 6.3014\n"
     ]
    }
   ],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.4)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_14_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10\n",
      "1000/1000 [==============================] - 706s 701ms/step - loss: 8.1382\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 705s 705ms/step - loss: 8.1154\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 701s 701ms/step - loss: 8.0926\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 8.0750\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 8.0577\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 8.0419\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 702s 702ms/step - loss: 8.0260\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 8.0136\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 7.9990\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 700s 700ms/step - loss: 7.9863\n"
     ]
    }
   ],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.5)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_15_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/50\n",
      "1000/1000 [==============================] - 709s 703ms/step - loss: 9.8972\n",
      "Epoch 1/50\n",
      " 542/1000 [===============>..............] - ETA: 5:25 - loss: 9.8802"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10520/949052681.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m           \u001b[0mxs_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m           \u001b[0mes_m\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnergy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m           \u001b[0mlog_p0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs_m\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m           \u001b[0mKL_D\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mes_m\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlog_p0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m           \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mae_tape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \"\"\"\n\u001b[1;32m-> 1296\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py\u001b[0m in \u001b[0;36m_call_log_prob\u001b[1;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_log_prob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_prob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\distributions\\transformed_distribution.py\u001b[0m in \u001b[0;36m_log_prob\u001b[1;34m(self, y, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m         self.event_shape)\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m     ildj = self.bijector.inverse_log_det_jacobian(\n\u001b[0m\u001b[0;32m    349\u001b[0m         y, event_ndims=event_ndims, **bijector_kwargs)\n\u001b[0;32m    350\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbijector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_injective\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py\u001b[0m in \u001b[0;36minverse_log_det_jacobian\u001b[1;34m(self, y, event_ndims, name, **kwargs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mevent_ndims\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mbijector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \"\"\"\n\u001b[1;32m-> 1320\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_inverse_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_forward_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36m_call_inverse_log_det_jacobian\u001b[1;34m(self, y, event_ndims, name, **kwargs)\u001b[0m\n\u001b[0;32m    464\u001b[0m       event_ndims = nest_util.coerce_structure(\n\u001b[0;32m    465\u001b[0m           self.inverse_min_event_ndims, event_ndims)\n\u001b[1;32m--> 466\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inverse_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    467\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_inverse_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36m_inverse_log_det_jacobian\u001b[1;34m(self, y, event_ndims, **kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[0mincreased_dof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_walk_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mincreased_dof\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0massertions\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldj_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ildj'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36m_call_walk_inverse\u001b[1;34m(self, step_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mpack_structs_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_min_event_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m     packed_result = self._walk_inverse(\n\u001b[0m\u001b[0;32m    346\u001b[0m         transform_wrapper, packed_args, **kwargs)\n\u001b[0;32m    347\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0munpack_structs_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_min_event_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpacked_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\chain.py\u001b[0m in \u001b[0;36m_walk_inverse\u001b[1;34m(self, step_fn, y, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;34m\"\"\"Applies `transform_fn` to `y` sequentially over nested bijectors.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbij\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bijectors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m       \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m  \u001b[1;31m# Now `x`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36mtransform_wrapper\u001b[1;34m(bij, packed_ys, **nested)\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpacked_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnested\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m       \u001b[0mys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpack_structs_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_min_event_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpacked_ys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m       \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnested\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mpack_structs_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_min_event_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(bij, y, y_event_ndims, increased_dof, **kwargs)\u001b[0m\n\u001b[0;32m    477\u001b[0m       \u001b[1;31m# Compute the LDJ for this step, and add it to the rolling sum.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m       component_ldj = tf.convert_to_tensor(\n\u001b[1;32m--> 479\u001b[1;33m           \u001b[0mbij\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_event_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    480\u001b[0m           dtype_hint=ldj_sum.dtype)\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py\u001b[0m in \u001b[0;36minverse_log_det_jacobian\u001b[1;34m(self, y, event_ndims, name, **kwargs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mevent_ndims\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvalid\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mbijector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \"\"\"\n\u001b[1;32m-> 1320\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_inverse_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_forward_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_ndims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py\u001b[0m in \u001b[0;36m_call_inverse_log_det_jacobian\u001b[1;34m(self, y, event_ndims, name, **kwargs)\u001b[0m\n\u001b[0;32m   1263\u001b[0m           \u001b[0mildj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ildj'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1264\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_inverse_log_det_jacobian'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1265\u001b[1;33m           \u001b[0mildj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ildj'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inverse_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1266\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_forward_log_det_jacobian'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1267\u001b[0m           \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Fall back to computing `-fldj(x)`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10520/2523341817.py\u001b[0m in \u001b[0;36m_inverse_log_det_jacobian\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_inverse_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_forward_log_det_jacobian\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10520/2523341817.py\u001b[0m in \u001b[0;36m_inverse\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[0my1D_imag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1D_imag\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0myD\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mirfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomplex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1D_real\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my1D_imag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\ops\\signal\\fft_ops.py\u001b[0m in \u001b[0;36m_irfft\u001b[1;34m(input_tensor, fft_length, name)\u001b[0m\n\u001b[0;32m    167\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mfft_length_static\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mfft_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfft_length_static\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mifft_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfft_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTreal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreal_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m   \u001b[0m_irfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"    Treal.*?\\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mifft_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0m_irfft\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_spectral_ops.py\u001b[0m in \u001b[0;36mirfft\u001b[1;34m(input, fft_length, Treal, name)\u001b[0m\n\u001b[0;32m    899\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    902\u001b[0m         _ctx, \"IRFFT\", name, input, fft_length, \"Treal\", Treal)\n\u001b[0;32m    903\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(1.6)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 50\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "  \n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint_directory = './'\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_16_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100\n",
      " 557/1000 [===============>..............] - ETA: 5:21 - loss: 18.1240"
     ]
    }
   ],
   "source": [
    "#E=lambda*(x**2-f**2)**2 so f is the shift between minima \n",
    "#Start from one and increasy up to 5 \n",
    "@tf.function\n",
    "def Energy(y):\n",
    "        lamd = 1.\n",
    "        f = np.sqrt(2)\n",
    "        a = 0.1\n",
    "        m = 0.5\n",
    "        kinetic_energy = tf.reduce_sum( m / ( 2 * a ) * (y-tf.roll(y,shift=-1,axis=1))**2,axis=-1)\n",
    "        potential_energy = tf.reduce_sum(lamd * ( y**2 - f**2 )**2,axis=-1)\n",
    "        return kinetic_energy + a*potential_energy\n",
    "    \n",
    "    \n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "n_iter = 1000\n",
    "n_samples = 4096 \n",
    "#KLD xs is configuration Energy is the energy of this configuration log_p0 KL_D is some constants to which to grad is applied\n",
    "#More or less they normalize the value and make procedure somehow more stable\n",
    "#put them to zero for standart KLD                                \n",
    "@tf.function\n",
    "def KL_Divergence(xs,energy,log_p0,KL_D):\n",
    "    log_p1 = flow2.log_prob(xs)\n",
    "    KL_D1 = (log_p1+energy-KL_D)*tf.exp(log_p1-log_p0)/tf.abs(KL_D)\n",
    "    KL_D = KL_D1\n",
    "    return tf.math.reduce_mean(KL_D)\n",
    "checkpoint_directory=\"C:\\FFT\\\\\"\n",
    "#these are optimizer and params\n",
    "lr_decay = .1\n",
    "learning_rate = .0001\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=flow2)\n",
    "\n",
    "#status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print('Epoch {:}/{:}'.format(epoch, n_epochs))\n",
    "    checkpoint.save(checkpoint_directory+'ckpt_30_144_fft_{}'.format(epoch))  \n",
    "    progbar = Progbar(n_iter)\n",
    "    for iter in range(n_iter):\n",
    "        accum_gradient = [tf.zeros_like(this_var) for this_var in flow2.trainable_variables]\n",
    "        for k in range(1):\n",
    "          xs_m = flow2.sample(n_samples)\n",
    "          es_m = Energy(xs_m)\n",
    "          log_p0 = flow2.log_prob(xs_m)\n",
    "          KL_D=tf.reduce_mean(es_m+log_p0)\n",
    "          with tf.GradientTape() as ae_tape:\n",
    "              loss = KL_Divergence(xs_m,es_m,log_p0,KL_D)\n",
    "          \n",
    "          gradients = ae_tape.gradient(loss, flow2.trainable_variables)\n",
    "          accum_gradient = [(acum_grad+grad) for acum_grad, grad in zip(accum_gradient, gradients)]\n",
    "        \n",
    "        accum_gradient = [this_grad/10. for this_grad in accum_gradient]\n",
    "        optimizer.apply_gradients(zip(accum_gradient, flow2.trainable_variables))\n",
    "        progbar.add(1, values=[('loss', KL_D)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs=flow2.sample(5*10**4).numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "Xs=xs[:,60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3.47623543e-04, 5.21436395e-04, 1.39049417e-03, 4.34529429e-03,\n",
       "        1.39049417e-02, 3.16338079e-02, 5.77055081e-02, 1.07068051e-01,\n",
       "        1.80242994e-01, 2.92525514e-01, 4.35919923e-01, 5.53243442e-01,\n",
       "        6.64482402e-01, 7.26533957e-01, 7.71724265e-01, 7.63903527e-01,\n",
       "        7.38526599e-01, 6.50403986e-01, 5.65931421e-01, 4.76070689e-01,\n",
       "        3.89686395e-01, 2.98087265e-01, 2.31691271e-01, 1.78157158e-01,\n",
       "        1.37658994e-01, 1.14020589e-01, 7.75200952e-02, 5.70102979e-02,\n",
       "        4.25839061e-02, 3.45885694e-02, 2.78098978e-02, 1.49478201e-02,\n",
       "        1.21668303e-02, 1.14715829e-02, 7.64772587e-03, 4.17148251e-03,\n",
       "        5.38817050e-03, 3.47623543e-03, 1.73811952e-03, 5.21435314e-04,\n",
       "        1.39049561e-03, 6.95247806e-04, 6.95247086e-04, 3.47623543e-04,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.73811771e-04, 1.73811771e-04,\n",
       "        1.73812132e-04, 1.73811771e-04]),\n",
       " array([-2.9333918 , -2.8183248 , -2.703258  , -2.588191  , -2.473124  ,\n",
       "        -2.358057  , -2.2429903 , -2.1279233 , -2.0128562 , -1.8977894 ,\n",
       "        -1.7827225 , -1.6676555 , -1.5525886 , -1.4375216 , -1.3224547 ,\n",
       "        -1.2073877 , -1.0923208 , -0.97725385, -0.8621869 , -0.74711996,\n",
       "        -0.632053  , -0.51698613, -0.40191916, -0.28685224, -0.1717853 ,\n",
       "        -0.05671835,  0.05834859,  0.17341553,  0.28848246,  0.4035494 ,\n",
       "         0.5186163 ,  0.63368326,  0.7487502 ,  0.86381716,  0.9788841 ,\n",
       "         1.093951  ,  1.209018  ,  1.3240849 ,  1.4391519 ,  1.5542188 ,\n",
       "         1.6692858 ,  1.7843527 ,  1.8994195 ,  2.0144866 ,  2.1295536 ,\n",
       "         2.2446203 ,  2.3596873 ,  2.4747543 ,  2.5898213 ,  2.704888  ,\n",
       "         2.819955  ], dtype=float32),\n",
       " <BarContainer object of 50 artists>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(Xs,density=True,bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17896/2375719856.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mXs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "Xs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acceptance(energy_change):\n",
    "    probability = np.exp(-energy_change)\n",
    "    acceptance = np.random.uniform() < probability\n",
    "    return acceptance\n",
    "\n",
    "def swipe(start_x):\n",
    "    x = start_x\n",
    "    new_x = flow2.sample(1)\n",
    "    d_log_p = flow2.log_prob(new_x)-flow2.log_prob(x)\n",
    "\n",
    "    delta_e = Energy(new_x)-Energy(x)\n",
    "    if acceptance(delta_e):\n",
    "        x=new_x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_num = 2000\n",
    "xs = np.zeros((step_num,144),dtype=np.float32)\n",
    "def training_loop(step_num):\n",
    "    x = flow2.sample(1).numpy()\n",
    "    for step in np.arange(0,step_num):\n",
    "        x = swipe(x)\n",
    "        xs[step] = x[0]\n",
    "        if step%1000 == 0:\n",
    "            print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17896/1746923968.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17896/3197133002.py\u001b[0m in \u001b[0;36mtraining_loop\u001b[1;34m(step_num)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstep_num\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17896/3381505824.py\u001b[0m in \u001b[0;36mswipe\u001b[1;34m(start_x)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mnew_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0md_log_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mflow2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdelta_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnergy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_x\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mEnergy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py\u001b[0m in \u001b[0;36mlog_prob\u001b[1;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[0;32m   1294\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m     \"\"\"\n\u001b[1;32m-> 1296\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\distributions\\distribution.py\u001b[0m in \u001b[0;36m_call_log_prob\u001b[1;34m(self, value, name, **kwargs)\u001b[0m\n\u001b[0;32m   1276\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_and_control_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_log_prob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_prob'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\distributions\\transformed_distribution.py\u001b[0m in \u001b[0;36m_log_prob\u001b[1;34m(self, y, **kwargs)\u001b[0m\n\u001b[0;32m    343\u001b[0m     event_ndims = tf.nest.map_structure(\n\u001b[0;32m    344\u001b[0m         \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank_from_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_shape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m         self.event_shape)\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\distributions\\transformed_distribution.py\u001b[0m in \u001b[0;36m_event_shape_tensor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_event_shape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m     return self.bijector.forward_event_shape_tensor(\n\u001b[0m\u001b[0;32m    270\u001b[0m         self.distribution.event_shape_tensor())\n\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py\u001b[0m in \u001b[0;36mforward_event_shape_tensor\u001b[1;34m(self, input_shape, name)\u001b[0m\n\u001b[0;32m    977\u001b[0m           self.inverse_min_event_ndims, tf.int32)\n\u001b[0;32m    978\u001b[0m       return nest_util.convert_to_nested_tensor(\n\u001b[1;32m--> 979\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_event_shape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m           \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_shape_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m           name='output_event_shape', allow_packing=True)\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36m_forward_event_shape_tensor\u001b[1;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_forward_event_shape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m     return self._call_walk_forward(\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_event_shape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         x, **kwargs)\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36m_call_walk_forward\u001b[1;34m(self, step_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_walk_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m     \u001b[1;31m# Convert a tuple of structures to a structure of tuples. This\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\chain.py\u001b[0m in \u001b[0;36m_walk_forward\u001b[1;34m(self, step_fn, x, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;34m\"\"\"Applies `transform_fn` to `x` sequentially over nested bijectors.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbij\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bijectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbij\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m  \u001b[1;31m# Now `y`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\composition.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(b, x, **kwds)\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_forward_event_shape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     return self._call_walk_forward(\n\u001b[1;32m--> 571\u001b[1;33m         \u001b[1;32mlambda\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_event_shape_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    572\u001b[0m         x, **kwargs)\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow_probability\\python\\bijectors\\bijector.py\u001b[0m in \u001b[0;36mforward_event_shape_tensor\u001b[1;34m(self, input_shape, name)\u001b[0m\n\u001b[0;32m    971\u001b[0m           name='input_event_shape', allow_packing=True)\n\u001b[0;32m    972\u001b[0m       \u001b[1;31m# Wrap inputs in identity to make sure control_scope is respected.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m       \u001b[0minput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m       \u001b[1;31m# Refer to static-dtype to get structure; we don't care about ntype here.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 914\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 914\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m   \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m   \u001b[1;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_handle_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[1;34m(input, name)\u001b[0m\n\u001b[0;32m   4061\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4062\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4063\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   4064\u001b[0m         _ctx, \"Identity\", name, input)\n\u001b[0;32m   4065\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop(step_num)\n",
    "xs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(xs[:,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 144)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
